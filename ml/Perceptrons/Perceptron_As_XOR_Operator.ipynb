{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons as *XOR* operator\n",
    "\n",
    "This notebook implements *perceptrons* as the *XOR* logical function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Perceptron?\n",
    "\n",
    "Going by the definition, a *perceptron* is an algorithm used for **supervised learning** of **binary classifiers**.\n",
    "\n",
    "\n",
    "\n",
    "## How does a Perceptron work?\n",
    "\n",
    "A *Perceptron* is basically a simplified **neural network**. Following is the stepwise implementation:\n",
    "\n",
    "1. The input is taken as an N-dimensional vector x, the **input vector**.\n",
    "2. The input vector is multiplied by the parameter vector w, the **weights vector**.\n",
    "3. The corresponding **biases** are added. The sum of the weighted inputs and the bias are sometimes referred to as the **induced local field** and generally represented by *v* = w*x + b.\n",
    "4. The *induced local field*, *v* is then applied to the **activation function**, producing the perceptrons output, *y_hat*.\n",
    "\n",
    "Following is the computational graph of our *perceptron*:\n",
    "<img src=\"https://miro.medium.com/max/576/1*GRubAZEl0qmrD4u4mCC0Ng.png\">\n",
    "\n",
    "The Î£ symbol represents the linear combination of the inputs *x* by means of the weights *w* and the bias *b*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons as binary classifiers\n",
    "\n",
    "Perceptrons can be efficiently used as binary classifiers by proper manipulation of the **activation function**. They can hence be used to implement all possible **logical operations**. \n",
    "\n",
    "## Fundamental Logical Functions and Perceptrons\n",
    "\n",
    "A single *perceptron* with a properly manipulated *activation function* can efficiently implement any of the **three** fundamental logical functions: **NOT**, **AND** and **OR**.\n",
    "\n",
    "These three logical functions are claimed as fundamental since all other logical functions, no matter how complex, can be obtained by the combination of these three.\n",
    "\n",
    "## Complex Logical Functions and Perceptrons\n",
    "\n",
    "A combination of the fundamental logical operations can give any complex logical function. And a **single** perceptron can implement any of the fundamental logical functions. Hence, a proper combination of **multiple perceptrons** can be used to implement other logical functions like **XOR** etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the **XOR** Logical Function\n",
    "\n",
    "## Let's start from the basics\n",
    "\n",
    "How can we properly connect the **fundamental logical perceptrons** to implement the **XOR** function?\n",
    "\n",
    "For the binary inputs x1, x2 (x1 & x2 have values either 0 or 1), **XOR(x1, x2)** can be implemented as:\n",
    "<img src=\"https://miro.medium.com/max/875/1*B7j9TH-cCOEpYJzBT5T5zw.png\">\n",
    "\n",
    "## Following is the computational graph for this implementation\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/314/1*Rm1Cd2KDoi1ACE-5KFP_Iw.png\">\n",
    "\n",
    "### We will now implement this in our code..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(v):\n",
    "    '''\n",
    "    Inputs: v -> the induced local field, a scalar.\n",
    "    \n",
    "    We are using the Heaviside Step Function.\n",
    "    \n",
    "    For input v >= 0, it returns 1\n",
    "    \n",
    "    And for v < 0, it returns 0\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if v >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Perceptron Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_percep(x, w=0, b=0):\n",
    "    '''\n",
    "    Inputs: x -> a vector\n",
    "            w -> weights, a vector\n",
    "            b -> bias, a vector\n",
    "    \n",
    "    \n",
    "    Implements a simple perceptron with weight vector w and bias b\n",
    "    \n",
    "    The default values of both w and b are 0\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    v = np.dot(w, x) + b  # dimensions of w, x and b should be proper\n",
    "    \n",
    "    y_hat = activation_function(v)  # output of the perceptron \n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOT Logical Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Percep_as_NOT(x):\n",
    "    '''\n",
    "    \n",
    "    Inputs: x -> a vector\n",
    "    \n",
    "    Performs the NOT logical operation on the input x with\n",
    "    weight vector w and bias b\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Choose appropriate combination of w and b to implement the NOT function\n",
    "    # We have chosen w = -1, b = 0.5\n",
    "    # There can be infinite possible combinations of w and b to implement the NOT function\n",
    "    \n",
    "    w = -1\n",
    "    b = 0.5\n",
    "    \n",
    "    return simple_percep(x, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Percep_as_NOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving ahead, lets verify the Percep_as_NOT function implementation.\n",
    "\n",
    "Run the below cell and verify the output:\n",
    "\n",
    "#### Expected Output:\n",
    "\n",
    "Percep_as_NOT(0) = 1\n",
    "\n",
    "Percep_as_NOT(1) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percep_as_NOT(0) = 1\n",
      "Percep_as_NOT(1) = 0\n"
     ]
    }
   ],
   "source": [
    "# Percep_as_NOT(0) = 1\n",
    "print(\"Percep_as_NOT(0) = {}\".format(Percep_as_NOT(0)))\n",
    "\n",
    "# Percep_as_NOT(0) = 0\n",
    "print(\"Percep_as_NOT(1) = {}\".format(Percep_as_NOT(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our **Percep_as_NOT** function is working great.\n",
    "\n",
    "We will now implement the **AND** logical function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND Logical Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Percep_as_AND(x):\n",
    "    '''\n",
    "    Inputs: x -> a vector, containing x1 and x2\n",
    "    \n",
    "    Performs the AND logical operation on the input x with\n",
    "    weight vector w and bias b\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Choose appropriate combination of w and b to implement the AND function\n",
    "    # We have chosen w = [1, 1], b = -1.5\n",
    "    # There can be infinite possible combinations of w and b to implement the AND function\n",
    "    \n",
    "    w = np.array([1, 1])\n",
    "    b = -1.5\n",
    "    \n",
    "    return simple_percep(x, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Percep_as_AND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving ahead, lets verify the Percep_as_AND function implementation.\n",
    "\n",
    "Run the below cell and verify the output:\n",
    "\n",
    "#### Expected Output:\n",
    "\n",
    "Percep_as_AND([1, 1]) = 1\n",
    "\n",
    "Percep_as_AND([1, 0]) = 0\n",
    "\n",
    "Percep_as_AND([0, 1]) = 0\n",
    "\n",
    "Percep_as_AND([0, 0]) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percep_as_AND([1 1]) = 1\n",
      "Percep_as_AND([1 0]) = 0\n",
      "Percep_as_AND([0 1]) = 0\n",
      "Percep_as_AND([0 0]) = 0\n"
     ]
    }
   ],
   "source": [
    "# Percep_as_AND([1, 1]) = 1\n",
    "x = np.array([1, 1])\n",
    "print(\"Percep_as_AND({}) = {}\".format(x, Percep_as_AND(x)))\n",
    "\n",
    "# Percep_as_AND([1, 0]) = 0\n",
    "x = np.array([1, 0])\n",
    "print(\"Percep_as_AND({}) = {}\".format(x, Percep_as_AND(x)))\n",
    "\n",
    "# Percep_as_AND([0, 1]) = 0\n",
    "x = np.array([0, 1])\n",
    "print(\"Percep_as_AND({}) = {}\".format(x, Percep_as_AND(x)))\n",
    "\n",
    "# Percep_as_AND([0, 0]) = 0\n",
    "x = np.array([0, 0])\n",
    "print(\"Percep_as_AND({}) = {}\".format(x, Percep_as_AND(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our **Percep_as_AND** function is working great.\n",
    "\n",
    "We will now implement the **OR** logical function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR Logical Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Percep_as_OR(x):\n",
    "    '''\n",
    "    Inputs: x -> a vector, containing x1 and x2\n",
    "    \n",
    "    Performs the OR logical operation on the input x with\n",
    "    weight vector w and bias b\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Choose appropriate combination of w and b to implement the AND function\n",
    "    # We have chosen w = [1, 1], b = -0.5\n",
    "    # There can be infinite possible combinations of w and b to implement the AND function\n",
    "    \n",
    "    w = np.array([1, 1])\n",
    "    b = -0.5\n",
    "    \n",
    "    return simple_percep(x, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Percep_as_AND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving ahead, lets verify the Percep_as_OR function implementation.\n",
    "\n",
    "Run the below cell and verify the output:\n",
    "\n",
    "#### Expected Output:\n",
    "\n",
    "Percep_as_OR([1, 1]) = 1\n",
    "\n",
    "Percep_as_OR([1, 0]) = 1\n",
    "\n",
    "Percep_as_OR([0, 1]) = 1\n",
    "\n",
    "Percep_as_OR([0, 0]) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percep_as_OR([1 1]) = 1\n",
      "Percep_as_OR([1 0]) = 1\n",
      "Percep_as_OR([0 1]) = 1\n",
      "Percep_as_OR([0 0]) = 0\n"
     ]
    }
   ],
   "source": [
    "# Percep_as_OR([1, 1]) = 1\n",
    "x = np.array([1, 1])\n",
    "print(\"Percep_as_OR({}) = {}\".format(x, Percep_as_OR(x)))\n",
    "\n",
    "# Percep_as_OR([1, 0]) = 1\n",
    "x = np.array([1, 0])\n",
    "print(\"Percep_as_OR({}) = {}\".format(x, Percep_as_OR(x)))\n",
    "\n",
    "# Percep_as_OR([0, 1]) = 1\n",
    "x = np.array([0, 1])\n",
    "print(\"Percep_as_OR({}) = {}\".format(x, Percep_as_OR(x)))\n",
    "\n",
    "# Percep_as_OR([0, 0]) = 0\n",
    "x = np.array([0, 0])\n",
    "print(\"Percep_as_OR({}) = {}\".format(x, Percep_as_OR(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our **Percep_as_OR** function is working great.\n",
    "\n",
    "We will now use these fundamental logical functions to implement the **XOR** logical function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR Logical Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Percep_as_XOR(x):\n",
    "    '''\n",
    "    This function implements the XOR logical function\n",
    "    \n",
    "    Inputs: x -> a vector, containing x1 and x2\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # XOR(x1, x2) = AND(NOT(AND(x1, x2)), OR(x1, x2))\n",
    "    \n",
    "    AND_x1_x2 = Percep_as_AND(x)\n",
    "    \n",
    "    NOT_AND_x1_x2 = Percep_as_NOT(AND_x1_x2)\n",
    "    \n",
    "    OR_x1_x2 = Percep_as_OR(x)\n",
    "    \n",
    "    # XOR(x1, x2) = AND(NOT(AND(x1, x2)), OR(x1, x2))\n",
    "    \n",
    "    return Percep_as_AND(np.array([NOT_AND_x1_x2, OR_x1_x2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Percep_as_XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully implemented the **XOR Logical Function** from scratch.\n",
    "\n",
    "Let's now verify its implementation.\n",
    "\n",
    "Run the below cell and verify the output:\n",
    "\n",
    "#### Expected Output:\n",
    "\n",
    "Percep_as_XOR([1, 1]) = 0\n",
    "\n",
    "Percep_as_XOR([1, 0]) = 1\n",
    "\n",
    "Percep_as_XOR([0, 1]) = 1\n",
    "\n",
    "Percep_as_XOR([0, 0]) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percep_as_XOR([1 1]) = 0\n",
      "Percep_as_XOR([1 0]) = 1\n",
      "Percep_as_XOR([0 1]) = 1\n",
      "Percep_as_XOR([0 0]) = 0\n"
     ]
    }
   ],
   "source": [
    "# Percep_as_XOR([1, 1]) = 0\n",
    "x = np.array([1, 1])\n",
    "print(\"Percep_as_XOR({}) = {}\".format(x, Percep_as_XOR(x)))\n",
    "\n",
    "# Percep_as_XOR([1, 0]) = 1\n",
    "x = np.array([1, 0])\n",
    "print(\"Percep_as_XOR({}) = {}\".format(x, Percep_as_XOR(x)))\n",
    "\n",
    "# Percep_as_XOR([0, 1]) = 1\n",
    "x = np.array([0, 1])\n",
    "print(\"Percep_as_XOR({}) = {}\".format(x, Percep_as_XOR(x)))\n",
    "\n",
    "# Percep_as_XOR([0, 0]) = 0\n",
    "x = np.array([0, 0])\n",
    "print(\"Percep_as_XOR({}) = {}\".format(x, Percep_as_XOR(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hurray! We have successfully implemented the XOR Logical Function using Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things we learnt:\n",
    "\n",
    "1. Perceptrons as a simplified neural network.\n",
    "2. Perceptrons as the fundamental Logical Operators.\n",
    "3. Combining Perceptrons to implement the XOR Logical Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "\n",
    "#### Supervised Learning:  \n",
    "https://en.wikipedia.org/wiki/Supervised_learning\n",
    "\n",
    "#### Binary Classification:\n",
    "https://en.wikipedia.org/wiki/Binary_classification\n",
    "\n",
    "#### Neural Network:\n",
    "https://en.wikipedia.org/wiki/Neural_network\n",
    "\n",
    "#### Perceptrons\n",
    "https://en.wikipedia.org/wiki/Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project:    Algobook\n",
    "Github Link: https://github.com/geekquad/AlgoBook\n",
    "\n",
    "#### Author:    Aayush Pandey\n",
    "Github: A-Pandey20\n",
    "\n",
    "https://github.com/A-Pandey20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
